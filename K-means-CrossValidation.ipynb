{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da3137-c7aa-4b36-912d-cb9feb8cd6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb28a947-01e7-4816-83c7-802091f0ef38",
   "metadata": {},
   "source": [
    "# Dataset split K-means cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d481e69-6a32-43e0-ba6c-d8cfc0f1415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c0abdc-502d-497b-a8d1-ff0b7224c03a",
   "metadata": {},
   "source": [
    "## Load paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e8400-97cb-4adb-ab73-d4a0b64a24d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "root = os.path.join('datasets', 'volcanic_eruptions')\n",
    "\n",
    "eruption_paths = glob.glob(os.path.join(os.path.join(root, 'eruption'),'*'))\n",
    "no_eruption_paths = glob.glob(os.path.join(os.path.join(root, 'no_eruption'),'*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc9c474-7aa0-4f65-8d63-b9a369c2e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = np.concatenate((eruption_paths, no_eruption_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd066e4-b25d-45e9-a581-295a00fac703",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad555dd4-9f4f-4085-b2be-2c75cb100abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def split(paths, K):\n",
    "    random.shuffle(paths)\n",
    "    \n",
    "    len_k = int(len(paths)//K)\n",
    "    print(len(paths), len_k)\n",
    "    \n",
    "    datasets = []\n",
    "    \n",
    "    for i in range(K):\n",
    "        datasets.append([paths[i*len_k:(i+1)*len_k]])\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389cdc56-ffb3-40ad-b44b-94ee1da79635",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = split(paths, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4cab36-2f1a-4a84-bece-c8dde448f375",
   "metadata": {},
   "source": [
    "## Create CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6abab7-a86a-49d5-bde4-d6fd4bc9c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnModel import cnnModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f329e5-ffa7-4fd4-a018-ca8047a2820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(K):\n",
    "    \n",
    "    train_set = np.concatenate(np.concatenate(np.delete(np.array(datasets), 0, axis = 0)))\n",
    "    val_set =  datasets[k][0]\n",
    "    \n",
    "    model = cnnModel('{}-fold-cross-validation'.format(k), train_set,val_set)\n",
    "    print(' -----------------  Fold {}  -----------------'.format(k))\n",
    "    _ = model.train(epochs = 100, batch_size = 16)\n",
    "    ground_truths, predictions, confusion_matrix, norm_confusion_matrix = model.test()\n",
    "    \n",
    "    print(confusion_matrix)\n",
    "    print(norm_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c70ff-7690-42b7-972a-cb9a1e80465a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
